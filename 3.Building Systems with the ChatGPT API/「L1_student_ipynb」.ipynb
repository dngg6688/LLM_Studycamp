{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dngg6688/LLM_Studycamp/blob/main/3.Building%20Systems%20with%20the%20ChatGPT%20API/%E3%80%8CL1_student_ipynb%E3%80%8D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae5bcee9-6588-4d29-bbb9-6fb351ef6630",
      "metadata": {
        "id": "ae5bcee9-6588-4d29-bbb9-6fb351ef6630"
      },
      "source": [
        "# L1 Language Models, the Chat Format and Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c797991-8486-4d79-8c1d-5dc0c1289c2f",
      "metadata": {
        "id": "0c797991-8486-4d79-8c1d-5dc0c1289c2f"
      },
      "source": [
        "## Setup\n",
        "#### Load the API key and relevant Python libaries.\n",
        "In this course, we've provided some code that loads the OpenAI API key for you."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3sCxb7b3abO",
        "outputId": "34e7c01b-bb88-4d55-edcf-c0e19a8fa01e"
      },
      "id": "y3sCxb7b3abO",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m895.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8ViDsmW3mPU",
        "outputId": "d0930248-e652-4512-c3fd-1410d9771649"
      },
      "id": "S8ViDsmW3mPU",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "19cd4e96",
      "metadata": {
        "height": 132,
        "id": "19cd4e96"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import tiktoken\n",
        "# from dotenv import load_dotenv, find_dotenv\n",
        "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
        "from google.colab import userdata\n",
        "openai.api_key  = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47ba0938-7ca5-46c4-a9d1-b55708d4dc7c",
      "metadata": {
        "id": "47ba0938-7ca5-46c4-a9d1-b55708d4dc7c"
      },
      "source": [
        "#### helper function\n",
        "This may look familiar if you took the earlier course \"ChatGPT Prompt Engineering for Developers\" Course"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1ed96988",
      "metadata": {
        "height": 149,
        "id": "1ed96988"
      },
      "outputs": [],
      "source": [
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe10a390-2461-447d-bf8b-8498db404c44",
      "metadata": {
        "id": "fe10a390-2461-447d-bf8b-8498db404c44"
      },
      "source": [
        "## Prompt the model and get a completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e1cc57b2",
      "metadata": {
        "height": 30,
        "id": "e1cc57b2"
      },
      "outputs": [],
      "source": [
        "response = get_completion(\"What is the capital of France?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "76774108",
      "metadata": {
        "height": 30,
        "id": "76774108",
        "outputId": "53f22e82-11b1-49a7-b69f-c4fb0dc044d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is Paris.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b83d4e38-3e3c-4c5a-a949-040a27f29d63",
      "metadata": {
        "id": "b83d4e38-3e3c-4c5a-a949-040a27f29d63"
      },
      "source": [
        "## Tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cc2d9e40",
      "metadata": {
        "height": 64,
        "id": "cc2d9e40",
        "outputId": "cc56aebe-eb69-441c-a740-3676f34b1e9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The reversed letters of \"lollipop\" are \"pillipol\".\n"
          ]
        }
      ],
      "source": [
        "response = get_completion(\"Take the letters in lollipop \\\n",
        "and reverse them\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d2b14d0-749d-4a79-9812-7b00ace9ae6f",
      "metadata": {
        "id": "9d2b14d0-749d-4a79-9812-7b00ace9ae6f"
      },
      "source": [
        "\"lollipop\" in reverse should be \"popillol\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "37cab84f",
      "metadata": {
        "height": 47,
        "id": "37cab84f"
      },
      "outputs": [],
      "source": [
        "response = get_completion(\"\"\"Take the letters in \\\n",
        "l-o-l-l-i-p-o-p and reverse them\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1577c561",
      "metadata": {
        "height": 30,
        "id": "1577c561",
        "outputId": "5c463825-5454-4b4b-ba3d-7c8bd47bb0ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'p-o-p-i-l-l-o-l'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8b88940-d3ab-4c00-b5c0-31531deaacbd",
      "metadata": {
        "id": "c8b88940-d3ab-4c00-b5c0-31531deaacbd"
      },
      "source": [
        "## Helper function (chat format)\n",
        "Here's the helper function we'll use in this course."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8f89efad",
      "metadata": {
        "height": 200,
        "id": "8f89efad"
      },
      "outputs": [],
      "source": [
        "def get_completion_from_messages(messages,\n",
        "                                 model=\"gpt-3.5-turbo\",\n",
        "                                 temperature=0,\n",
        "                                 max_tokens=500):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature, # this is the degree of randomness of the model's output\n",
        "        max_tokens=max_tokens, # the maximum number of tokens the model can ouptut\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b28c3424",
      "metadata": {
        "height": 183,
        "id": "b28c3424",
        "outputId": "5e1d3415-51ba-4c7b-ff2a-92aa9e15f4f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oh, the carrot so orange and bright,\n",
            "With a smile that brings pure delight.\n",
            "It pokes its head out of the ground,\n",
            "In a happy veggie garden bound.\n",
            "\n",
            "It sways with joy, as gardeners pass,\n",
            "Each one in awe of its vibrant class.\n",
            "With every bite, it brings such glee,\n",
            "A healthy snack for you and me.\n",
            "\n",
            "So let us celebrate this joyous root,\n",
            "A happy carrot, oh how it's a hoot!\n",
            "In salads, soups, and dishes sweet,\n",
            "It always adds a colorful treat.\n"
          ]
        }
      ],
      "source": [
        "messages =  [\n",
        "{'role':'system',\n",
        " 'content':\"\"\"You are an assistant who\\\n",
        " responds in the style of Dr Seuss.\"\"\"},\n",
        "{'role':'user',\n",
        " 'content':\"\"\"write me a very short poem\\\n",
        " about a happy carrot\"\"\"},\n",
        "]\n",
        "response = get_completion_from_messages(messages, temperature=1)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "56c6978d",
      "metadata": {
        "height": 183,
        "id": "56c6978d",
        "outputId": "037625f0-f0b2-44c9-c1d8-3c0bebf7b5ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, there was a cheerful carrot named Charlie that lived in a bountiful vegetable garden.\n"
          ]
        }
      ],
      "source": [
        "# length\n",
        "messages =  [\n",
        "{'role':'system',\n",
        " 'content':'All your responses must be \\\n",
        "one sentence long.'},\n",
        "{'role':'user',\n",
        " 'content':'write me a story about a happy carrot'},\n",
        "]\n",
        "response = get_completion_from_messages(messages, temperature =1)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "14fd6331",
      "metadata": {
        "height": 217,
        "id": "14fd6331",
        "outputId": "4606ce22-7f6e-4289-e912-c28827bd56f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, in a garden so fine, there lived a happy carrot, oh so divine.\n"
          ]
        }
      ],
      "source": [
        "# combined\n",
        "messages =  [\n",
        "{'role':'system',\n",
        " 'content':\"\"\"You are an assistant who \\\n",
        "responds in the style of Dr Seuss. \\\n",
        "All your responses must be one sentence long.\"\"\"},\n",
        "{'role':'user',\n",
        " 'content':\"\"\"write me a story about a happy carrot\"\"\"},\n",
        "]\n",
        "response = get_completion_from_messages(messages,\n",
        "                                        temperature =1)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "89a70c79",
      "metadata": {
        "height": 370,
        "id": "89a70c79"
      },
      "outputs": [],
      "source": [
        "def get_completion_and_token_count(messages,\n",
        "                                   model=\"gpt-3.5-turbo\",\n",
        "                                   temperature=0,\n",
        "                                   max_tokens=500):\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "\n",
        "    content = response.choices[0].message[\"content\"]\n",
        "\n",
        "    token_dict = {\n",
        "'prompt_tokens':response['usage']['prompt_tokens'],\n",
        "'completion_tokens':response['usage']['completion_tokens'],\n",
        "'total_tokens':response['usage']['total_tokens'],\n",
        "    }\n",
        "\n",
        "    return content, token_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "a64cf3c6",
      "metadata": {
        "height": 166,
        "id": "a64cf3c6"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "{'role':'system',\n",
        " 'content':\"\"\"You are an assistant who responds\\\n",
        " in the style of Dr Seuss.\"\"\"},\n",
        "{'role':'user',\n",
        " 'content':\"\"\"write me a very short poem \\\n",
        " about a happy carrot\"\"\"},\n",
        "]\n",
        "response, token_dict = get_completion_and_token_count(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "cfd8fbd4",
      "metadata": {
        "height": 30,
        "id": "cfd8fbd4",
        "outputId": "a3e75109-4977-49e3-c259-0ccc5b22d2f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oh, the happy carrot, so bright and orange,\n",
            "With a smile so wide, it's hard to manage.\n",
            "In the garden it grew, with love and care,\n",
            "Bathing in sunshine, breathing in fresh air.\n",
            "\n",
            "Its leaves danced and swayed, in the gentle breeze,\n",
            "As the happy carrot grew, with such ease.\n",
            "With every day passing, it grew so tall,\n",
            "Spreading joy and happiness to all.\n",
            "\n",
            "From the ground it was pulled, with a joyful cheer,\n",
            "For the happy carrot had nothing to fear.\n",
            "It brought laughter and delight to every plate,\n",
            "As its sweet flavor made taste buds celebrate.\n",
            "\n",
            "So let us rejoice, for the happy carrot's glee,\n",
            "A symbol of joy and health, you see.\n",
            "In every bite, a smile will surely bloom,\n",
            "As the happy carrot brings happiness to the room.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "352ad320",
      "metadata": {
        "height": 30,
        "id": "352ad320",
        "outputId": "cb20a96d-18e5-4ab6-e52d-371942106502",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt_tokens': 36, 'completion_tokens': 166, 'total_tokens': 202}\n"
          ]
        }
      ],
      "source": [
        "print(token_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65372cdd-d869-4768-947a-0173e7f96335",
      "metadata": {
        "id": "65372cdd-d869-4768-947a-0173e7f96335"
      },
      "source": [
        "#### Notes on using the OpenAI API outside of this classroom\n",
        "\n",
        "To install the OpenAI Python library:\n",
        "```\n",
        "!pip install openai\n",
        "```\n",
        "\n",
        "The library needs to be configured with your account's secret key, which is available on the [website](https://platform.openai.com/account/api-keys).\n",
        "\n",
        "You can either set it as the `OPENAI_API_KEY` environment variable before using the library:\n",
        " ```\n",
        " !export OPENAI_API_KEY='sk-...'\n",
        " ```\n",
        "\n",
        "Or, set `openai.api_key` to its value:\n",
        "\n",
        "```\n",
        "import openai\n",
        "openai.api_key = \"sk-...\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8f889c1-f2e4-40a5-bd27-164facb54402",
      "metadata": {
        "id": "d8f889c1-f2e4-40a5-bd27-164facb54402"
      },
      "source": [
        "#### A note about the backslash\n",
        "- In the course, we are using a backslash `\\` to make the text fit on the screen without inserting newline '\\n' characters.\n",
        "- GPT-3 isn't really affected whether you insert newline characters or not.  But when working with LLMs in general, you may consider whether newline characters in your prompt may affect the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5df11dc",
      "metadata": {
        "height": 30,
        "id": "f5df11dc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}